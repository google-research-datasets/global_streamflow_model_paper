{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dzNFopjta4Ln"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 4538,
     "status": "ok",
     "timestamp": 1686657068813,
     "user": {
      "displayName": "Grey Nearing",
      "userId": "00389065855797486266"
     },
     "user_tz": -180
    },
    "id": "2mIv77EiNGS3"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 5599,
     "status": "ok",
     "timestamp": 1686657075040,
     "user": {
      "displayName": "Grey Nearing",
      "userId": "00389065855797486266"
     },
     "user_tz": -180
    },
    "id": "NKOTbNQx04-D"
   },
   "outputs": [],
   "source": [
    "from backend import data_paths\n",
    "from backend import evaluation_utils\n",
    "from backend import gauge_groups_utils\n",
    "from backend import loading_utils\n",
    "from backend import metrics_utils\n",
    "from backend import skill_prediction_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Basin Attributes for Latitude & Longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gauges = gauge_groups_utils.get_full_gauge_group()\n",
    "print(f'There are {len(gauges)} gauges.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = loading_utils.load_attributes_file(gauges=gauges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3DH4JNSQM7lD"
   },
   "source": [
    "# Load Return Period Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 82,
     "status": "ok",
     "timestamp": 1686657081409,
     "user": {
      "displayName": "Grey Nearing",
      "userId": "00389065855797486266"
     },
     "user_tz": -180
    },
    "id": "r-Ipfs2Z_YWv"
   },
   "outputs": [],
   "source": [
    "_DATASET_RETURN_PERIOD_METRICS_PATH = {\n",
    "    'google_2014': data_paths.GOOGLE_2014_RETURN_PERIOD_METRICS_DIR,\n",
    "    'google_1980': data_paths.GOOGLE_1980_RETURN_PERIOD_METRICS_DIR,\n",
    "    'glofas_2014': data_paths.GLOFAS_2014_RETURN_PERIOD_METRICS_DIR,\n",
    "    'glofas_1980': data_paths.GLOFAS_1980_RETURN_PERIOD_METRICS_DIR,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 6111,
     "status": "ok",
     "timestamp": 1686657097555,
     "user": {
      "displayName": "Grey Nearing",
      "userId": "00389065855797486266"
     },
     "user_tz": -180
    },
    "id": "7O3Y21AUxPxS",
    "outputId": "127befc9-bb5c-480c-d1e9-62a6e8ba9a08"
   },
   "outputs": [],
   "source": [
    "precisions_by_lead_time = {}\n",
    "recalls_by_lead_time = {}\n",
    "\n",
    "precisions_by_return_period = {}\n",
    "recalls_by_return_period = {}\n",
    "\n",
    "for dataset, data_path in _DATASET_RETURN_PERIOD_METRICS_PATH.items():\n",
    "    print(f'Working on {dataset} ...')\n",
    "    file_path = data_paths.CONCATENATED_RETURN_PERIOD_DICTS_DIR / f'{dataset}_return_period_dicts.pkl'\n",
    "    with open(file_path, 'rb') as f:\n",
    "        precisions_by_lead_time[dataset], recalls_by_lead_time[dataset] = pkl.load(f)\n",
    "    print(f'Finished loading {dataset}. \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate F1 Scores from Precision & Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 920,
     "status": "ok",
     "timestamp": 1686657100636,
     "user": {
      "displayName": "Grey Nearing",
      "userId": "00389065855797486266"
     },
     "user_tz": -180
    },
    "id": "5gE4-QHOCwSC"
   },
   "outputs": [],
   "source": [
    "f1s_by_lead_time = {\n",
    "    dataset: {\n",
    "        experiment: {\n",
    "            lead_time:\n",
    "              evaluation_utils.f1_from_precision_and_recall_dfs(\n",
    "                  precision_df=precisions_by_lead_time[dataset][experiment][lead_time],\n",
    "                  recall_df=recalls_by_lead_time[dataset][experiment][lead_time]\n",
    "              ) for lead_time in data_paths.LEAD_TIMES\n",
    "        } for experiment in precisions_by_lead_time[dataset]\n",
    "    } for dataset in _DATASET_RETURN_PERIOD_METRICS_PATH\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RBieOzakDzvz"
   },
   "source": [
    "# Skill Predictability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Basin Attributes as Classifier/Regression Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 657,
     "status": "ok",
     "timestamp": 1686133666250,
     "user": {
      "displayName": "Grey Nearing",
      "userId": "00389065855797486266"
     },
     "user_tz": -180
    },
    "id": "beH2_oGURQhr",
    "outputId": "2fe10216-2d4e-48aa-9b64-0b2f95ad20f7"
   },
   "outputs": [],
   "source": [
    "# A subset of attributes to use as predictor values. This selection is mostly\n",
    "# due to challenges in naming all the HydroATLAS variables, and the skill\n",
    "# is not affected by removing some of the landcover class variables.\n",
    "ATTRIBUTE_DESCRIPTIVE_NAMES = {\n",
    "    'calculated_drain_area': 'Drain Area',\n",
    "    'inu_pc_umn': 'Inundation Percent Min',\n",
    "    'inu_pc_umx': 'Inundation Percent Max',\n",
    "    # 'inu_pc_ult': 'Inundation Percent Long Term Maximum',\n",
    "    'lka_pc_use': 'Precent Lake Area',\n",
    "    'lkv_mc_usu': 'Lake Volume',\n",
    "    'rev_mc_usu': 'Reservoir Volume',\n",
    "    'ria_ha_usu': 'River Area',\n",
    "    'riv_tc_usu': 'River Volume',\n",
    "    'ele_mt_uav': 'Elevation',\n",
    "    'slp_dg_uav': 'Slope',\n",
    "    'tmp_dc_uyr': 'Air Temperature',\n",
    "    'pre_mm_uyr': 'Precipitation',\n",
    "    'pet_mm_uyr': 'PET',\n",
    "    'aet_mm_uyr': 'AET',\n",
    "    'ari_ix_uav': 'Aridity Index',\n",
    "    'cmi_ix_uyr': 'Climate Moisture Index',\n",
    "    'snw_pc_uyr': 'Snow Cover Extent',\n",
    "    'for_pc_use': 'Forest Cover Extent',\n",
    "    'crp_pc_use': 'Cropland Extent',\n",
    "    'pst_pc_use': 'Pastiure Extent',\n",
    "    'ire_pc_use': 'Irrigated Area Extent',\n",
    "    'gla_pc_use': 'Glacier Extent',\n",
    "    'prm_pc_use': 'Permafrost Extent',\n",
    "    'pac_pc_use': 'Protected Area Extent',\n",
    "    'cly_pc_uav': 'Soil Clay Fraction',\n",
    "    'slt_pc_uav': 'Soil Silt Fraction',\n",
    "    'snd_pc_uav': 'Soil Sand Fraction',\n",
    "    'soc_th_uav': 'Soil Organic Carbon',\n",
    "    'swc_pc_uyr': 'Soil Water Content',\n",
    "    'kar_pc_use': 'Karst Area Extent',\n",
    "    'ero_kh_uav': 'Soil Erosion',\n",
    "    'pop_ct_usu': 'Population Count',\n",
    "    'ppd_pk_uav': 'Population Density',\n",
    "    'urb_pc_use': 'Urban Area Extent',\n",
    "    'nli_ix_uav': 'Nighttime Lights Index',\n",
    "    # 'rdd_mk_uav': 'Road Density',\n",
    "    # 'hft_ix_u93': 'Human Footprint 1993',\n",
    "    # 'hft_ix_u09': 'Human Footprint 2009',\n",
    "    'gdp_ud_usu': 'GDP',\n",
    "    # 'latitude': 'latitude',\n",
    "    # 'longitude': 'longitude',\n",
    "}\n",
    "\n",
    "# Select the subset of attributes.\n",
    "regression_attributes = attributes[ATTRIBUTE_DESCRIPTIVE_NAMES.keys()]\n",
    "regression_attributes.rename(columns=ATTRIBUTE_DESCRIPTIVE_NAMES, inplace=True)\n",
    "\n",
    "# Normalize the attributes.\n",
    "regression_attributes = (regression_attributes - regression_attributes.mean()) / regression_attributes.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters for Reliability Score Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "H3_VhHwlG9l2"
   },
   "outputs": [],
   "source": [
    "# Hyperparamters for reliability score predictions.\n",
    "lead_time = 0\n",
    "return_period = 5\n",
    "metric='F1 Score'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nDzbJF50ZNN4"
   },
   "source": [
    "# Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "height": 740
    },
    "executionInfo": {
     "elapsed": 43037,
     "status": "error",
     "timestamp": 1686135236497,
     "user": {
      "displayName": "Grey Nearing",
      "userId": "00389065855797486266"
     },
     "user_tz": -180
    },
    "id": "3TtX33SxZRZF",
    "outputId": "97645296-6b62-483d-e61b-b771bcbd1f7a"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(\n",
    "    2, 1, \n",
    "    figsize=(\n",
    "        evaluation_utils.NATURE_FIG_SIZES['two_column'],\n",
    "        evaluation_utils.NATURE_FIG_SIZES['two_column'],\n",
    "    )\n",
    ")\n",
    "\n",
    "# --- GloFAS --------------------------------\n",
    "regression_data = f1s_by_lead_time['glofas_1980'][metrics_utils.GLOFAS_VARIABLE][\n",
    "    lead_time][return_period].dropna().rename(metric)\n",
    "experiment = metrics_utils.GLOFAS_VARIABLE\n",
    "bins = None #[-0.1, regression_data.mean(), 1.01]\n",
    "\n",
    "x, y = skill_prediction_utils.make_predictors(\n",
    "    x=regression_attributes,\n",
    "    y=regression_data,\n",
    "    metric=metric,\n",
    "    bins=bins\n",
    ")\n",
    "\n",
    "glofas_importances, _ = y_hat = skill_prediction_utils.feature_importances(\n",
    "    x, y,\n",
    "    # classifier=True\n",
    "    regression=True\n",
    ")\n",
    "\n",
    "skill_prediction_utils.plot_feature_importances(\n",
    "    importances=glofas_importances,\n",
    "    ax=axes[0]\n",
    ")\n",
    "axes[0].set_title(\n",
    "    evaluation_utils.EXPERIMENT_NAMES[experiment],\n",
    "    fontsize=evaluation_utils.NATURE_FONT_SIZES['title']\n",
    ")\n",
    "\n",
    "# --- Google Model ----------------------------\n",
    "regression_data = f1s_by_lead_time['google_1980']['kfold_splits'][\n",
    "    lead_time][return_period].dropna().rename(metric)\n",
    "experiment = 'kfold_splits'\n",
    "bins = None #[-0.1, regression_data.mean(), 1.01]\n",
    "\n",
    "x, y = skill_prediction_utils.make_predictors(\n",
    "    x=regression_attributes,\n",
    "    y=regression_data,\n",
    "    metric=metric,\n",
    "    bins=bins\n",
    ")\n",
    "\n",
    "google_importances, _ = y_hat = skill_prediction_utils.feature_importances(\n",
    "    x, y,\n",
    "    # classifier=True\n",
    "    regression=True\n",
    ")\n",
    "\n",
    "skill_prediction_utils.plot_feature_importances(\n",
    "    importances=google_importances,\n",
    "    ax=axes[1]\n",
    ")\n",
    "axes[1].set_title(\n",
    "    evaluation_utils.EXPERIMENT_NAMES[experiment], \n",
    "    fontsize=evaluation_utils.NATURE_FONT_SIZES['title']\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "evaluation_utils.save_figure(data_paths.PREDICTABILITY_FEATURE_IMPORTANCES_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "E58N2Y00dDYu"
   },
   "outputs": [],
   "source": [
    "# Choose top 5 most important features from both models.\n",
    "num_importances = 5\n",
    "top_features = list(set(glofas_importances.sort_values(ascending=False).index[:num_importances]).union(\n",
    "    set(google_importances.sort_values(ascending=False).index[:num_importances])))\n",
    "top_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q75J5yxBRZuI"
   },
   "source": [
    "## F1 Score Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "RTWckmBumbMu"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(\n",
    "    2, 2, \n",
    "    figsize=(\n",
    "            evaluation_utils.NATURE_FIG_SIZES['one_half_column'],\n",
    "            evaluation_utils.NATURE_FIG_SIZES['one_half_column']\n",
    "    )\n",
    ")\n",
    "\n",
    "gs = axes[0, 0].get_gridspec()\n",
    "for ax in axes[:, -1]:\n",
    "    ax.remove()\n",
    "bigax = fig.add_subplot(gs[:, -1])\n",
    "\n",
    "# --- GloFAS --------------------------------\n",
    "regression_data = f1s_by_lead_time['glofas_1980'][metrics_utils.GLOFAS_VARIABLE][\n",
    "    lead_time][return_period].dropna().rename(metric)\n",
    "experiment = metrics_utils.GLOFAS_VARIABLE\n",
    "bins = None# [-0.1, regression_data.mean(), 1.01]\n",
    "\n",
    "x, y = skill_prediction_utils.make_predictors(\n",
    "    x=regression_attributes,\n",
    "    y=regression_data,\n",
    "    metric=metric,\n",
    "    bins=bins\n",
    ")\n",
    "\n",
    "y_hat = skill_prediction_utils.train_kfold(\n",
    "    x, y,\n",
    "    # classifier=True\n",
    "    regression=True\n",
    ")\n",
    "\n",
    "bins = [-0.1, regression_data.mean(), 1.01]\n",
    "bin_names = [idx for idx, bin in enumerate(bins[1:])]\n",
    "y_cat = pd.cut(y, bins=bins, include_lowest=True, labels=bin_names)\n",
    "y_hat_cat = pd.cut(y_hat, bins=bins, include_lowest=True, labels=bin_names)\n",
    "\n",
    "print('------------------------------------')\n",
    "print('------------------------------------')\n",
    "print(experiment)\n",
    "print(precision_recall_fscore_support(y_cat, y_hat_cat, average='micro'))\n",
    "print('------------------------------------')\n",
    "print('------------------------------------')\n",
    "\n",
    "ax = axes[0, 0]\n",
    "cm = confusion_matrix(y_cat, y_hat_cat, normalize='true')\n",
    "sns.heatmap(cm, annot=True, ax=ax, cbar=False, fmt='.2g')\n",
    "ax.set_aspect('equal', 'box')\n",
    "ax.set_title(\n",
    "    f'{evaluation_utils.EXPERIMENT_NAMES[experiment]} (N = {y.shape[0]})',\n",
    "    fontsize=evaluation_utils.NATURE_FONT_SIZES['title']\n",
    ")\n",
    "ax.set_xlabel(\n",
    "    'Predicted Labels', \n",
    "    fontsize=evaluation_utils.NATURE_FONT_SIZES['axis_label']\n",
    ")\n",
    "ax.set_ylabel(\n",
    "    'True Labels', \n",
    "    fontsize=evaluation_utils.NATURE_FONT_SIZES['axis_label']\n",
    ")\n",
    "ax.xaxis.set_ticklabels(\n",
    "    ['Below Mean', 'Above Mean'],\n",
    "    fontsize=evaluation_utils.NATURE_FONT_SIZES['tick_labels']\n",
    ")\n",
    "ax.yaxis.set_ticklabels(\n",
    "    ['Below Mean', 'Above Mean'],\n",
    "    fontsize=evaluation_utils.NATURE_FONT_SIZES['tick_labels']\n",
    ")\n",
    "\n",
    "\n",
    "# --- Google Model ----------------------------\n",
    "regression_data = f1s_by_lead_time['google_1980']['kfold_splits'][\n",
    "    lead_time][return_period].dropna().rename(metric)\n",
    "experiment = 'kfold_splits'\n",
    "bins = None# [-0.1, regression_data.mean(), 1.01]\n",
    "\n",
    "x, y = skill_prediction_utils.make_predictors(\n",
    "    x=regression_attributes,\n",
    "    y=regression_data,\n",
    "    metric=metric,\n",
    "    bins=bins\n",
    ")\n",
    "\n",
    "y_hat = skill_prediction_utils.train_kfold(\n",
    "    x, y,\n",
    "    # classifier=True\n",
    "    regression=True\n",
    ")\n",
    "\n",
    "bins = [-0.1, regression_data.mean(), 1.01]\n",
    "bin_names = [idx for idx, bin in enumerate(bins[1:])]\n",
    "y_cat = pd.cut(y, bins=bins, include_lowest=True, labels=bin_names)\n",
    "y_hat_cat = pd.cut(y_hat, bins=bins, include_lowest=True, labels=bin_names)\n",
    "\n",
    "print('------------------------------------')\n",
    "print('------------------------------------')\n",
    "print(experiment)\n",
    "print(precision_recall_fscore_support(y_cat, y_hat_cat, average='micro'))\n",
    "print('------------------------------------')\n",
    "print('------------------------------------')\n",
    "\n",
    "ax = axes[1, 0]\n",
    "cm = confusion_matrix(y_cat, y_hat_cat, normalize='true')\n",
    "sns.heatmap(cm, annot=True, ax=ax, cbar=False, fmt='.2g')\n",
    "ax.set_aspect('equal', 'box')\n",
    "ax.set_title(\n",
    "    f'{evaluation_utils.EXPERIMENT_NAMES[experiment]} (N = {y.shape[0]})',\n",
    "    fontsize=evaluation_utils.NATURE_FONT_SIZES['title']\n",
    ")\n",
    "ax.set_xlabel(\n",
    "    'Predicted Labels', \n",
    "    fontsize=evaluation_utils.NATURE_FONT_SIZES['axis_label']\n",
    ")\n",
    "ax.set_ylabel(\n",
    "    'True Labels', \n",
    "    fontsize=evaluation_utils.NATURE_FONT_SIZES['axis_label']\n",
    ")\n",
    "ax.xaxis.set_ticklabels(\n",
    "    ['Below Mean', 'Above Mean'],\n",
    "    fontsize=evaluation_utils.NATURE_FONT_SIZES['tick_labels']\n",
    ")\n",
    "ax.yaxis.set_ticklabels(\n",
    "    ['Below Mean', 'Above Mean'],\n",
    "    fontsize=evaluation_utils.NATURE_FONT_SIZES['tick_labels']\n",
    ")\n",
    "\n",
    "# --- Attribute Correlations -----------------\n",
    "combined_data = pd.concat([\n",
    "    f1s_by_lead_time['glofas_1980'][metrics_utils.GLOFAS_VARIABLE][\n",
    "        lead_time][return_period].dropna().rename(metrics_utils.GLOFAS_VARIABLE),\n",
    "    f1s_by_lead_time['google_1980']['kfold_splits'][\n",
    "        lead_time][return_period].dropna().rename('kfold_splits')\n",
    "], axis=1).dropna()\n",
    "\n",
    "corr_matrix = pd.concat([regression_attributes, combined_data], axis=1).dropna()\n",
    "c = corr_matrix.corr()[[metrics_utils.GLOFAS_VARIABLE, 'kfold_splits']].drop(\n",
    "    [metrics_utils.GLOFAS_VARIABLE, 'kfold_splits'])\n",
    "cabs = c.abs()\n",
    "\n",
    "im = bigax.imshow(\n",
    "    c.loc[top_features, [metrics_utils.GLOFAS_VARIABLE, 'kfold_splits']], \n",
    "    cmap='PiYG',\n",
    "    vmin=-0.25, vmax=0.25,\n",
    ")\n",
    "bigax.set_xticks(\n",
    "    [0,1], \n",
    "    ['GloFAS', 'AI Model'],\n",
    "    fontsize=evaluation_utils.NATURE_FONT_SIZES['tick_labels']\n",
    ")\n",
    "bigax.set_yticks(\n",
    "    range(len(top_features)),\n",
    "    top_features,\n",
    "    fontsize=evaluation_utils.NATURE_FONT_SIZES['tick_labels']\n",
    ")\n",
    "cbar = plt.colorbar(im, ax=bigax)\n",
    "cbar.set_label(\n",
    "    'Correlations between F1 scores and Catchment Attributes', \n",
    "    rotation=90, \n",
    "    fontsize=evaluation_utils.NATURE_FONT_SIZES['axis_label']\n",
    ")\n",
    "cbar.ax.tick_params(labelsize=evaluation_utils.NATURE_FONT_SIZES['tick_labels']) \n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "evaluation_utils.save_figure(data_paths.PREDICATABILITY_CONFUSION_MATRICES_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "last_runtime": {
    "build_target": "//intelligence/flood_forecasting/colab:notebook",
    "kind": "private"
   },
   "provenance": [
    {
     "file_id": "1U5lV5oB6crQAV7-OLTxegU4Dathhwd8Q",
     "timestamp": 1678011930641
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
